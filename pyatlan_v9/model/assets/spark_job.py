# Auto-generated by PythonMsgspecRenderer.pkl - DO NOT EDIT
# SPDX-License-Identifier: Apache-2.0
# Copyright 2024 Atlan Pte. Ltd.

"""
SparkJob asset model with flattened inheritance.

This module provides:
- SparkJob: Flat asset class (easy to use)
- SparkJobAttributes: Nested attributes struct (extends AssetAttributes)
- SparkJobNested: Nested API format struct
"""

from __future__ import annotations

from typing import Union

from msgspec import UNSET, UnsetType

from pyatlan_v9.model.conversion_utils import (
    categorize_relationships,
    merge_relationships,
)
from pyatlan_v9.model.serde import Serde, get_serde
from pyatlan_v9.model.transform import register_asset

from .airflow_related import RelatedAirflow
from .asset import Asset, AssetAttributes, AssetNested, AssetRelationshipAttributes
from .catalog_related import RelatedCatalog
from .process_related import RelatedProcess

# =============================================================================
# FLAT ASSET CLASS
# =============================================================================


@register_asset
class SparkJob(Asset):
    """
    Instance of a Spark Job run in Atlan.
    """

    # Override type_name with SparkJob-specific default
    type_name: Union[str, UnsetType] = "SparkJob"

    spark_app_name: Union[str, None, UnsetType] = UNSET
    """Name of the Spark app containing this Spark Job For eg. extract_raw_data"""

    spark_master: Union[str, None, UnsetType] = UNSET
    """The Spark master URL eg. local, local[4], or spark://master:7077"""

    spark_run_version: Union[str, None, UnsetType] = UNSET
    """Spark Version for the Spark Job run eg. 3.4.1"""

    spark_run_open_lineage_version: Union[str, None, UnsetType] = UNSET
    """OpenLineage Version of the Spark Job run eg. 1.1.0"""

    spark_run_start_time: Union[int, None, UnsetType] = UNSET
    """Start time of the Spark Job eg. 1695673598218"""

    spark_run_end_time: Union[int, None, UnsetType] = UNSET
    """End time of the Spark Job eg. 1695673598218"""

    spark_run_open_lineage_state: Union[str, None, UnsetType] = UNSET
    """OpenLineage state of the Spark Job run eg. COMPLETE"""

    process: Union[RelatedProcess, None, UnsetType] = UNSET
    """"""

    inputs: Union[list[RelatedCatalog], None, UnsetType] = UNSET
    """"""

    outputs: Union[list[RelatedCatalog], None, UnsetType] = UNSET
    """"""

    spark_orchestrated_by_airflow_assets: Union[
        list[RelatedAirflow], None, UnsetType
    ] = UNSET
    """Airflow assets that execute this spark asset."""

    # =========================================================================
    # Optimized Serialization Methods (override Asset base class)
    # =========================================================================

    def to_json(self, nested: bool = True, serde: Serde | None = None) -> str:
        """
        Convert to JSON string using optimized nested struct serialization.

        Args:
            nested: If True (default), use nested API format. If False, use flat format.
            serde: Optional Serde instance for encoder reuse. Uses shared singleton if None.

        Returns:
            JSON string representation
        """
        if serde is None:
            serde = get_serde()
        if nested:
            return _spark_job_to_nested_bytes(self, serde).decode("utf-8")
        else:
            return serde.encode(self).decode("utf-8")

    @staticmethod
    def from_json(
        json_data: Union[str, bytes], serde: Serde | None = None
    ) -> "SparkJob":
        """
        Create from JSON string or bytes using optimized nested struct deserialization.

        Args:
            json_data: JSON string or bytes to deserialize
            serde: Optional Serde instance for decoder reuse. Uses shared singleton if None.

        Returns:
            SparkJob instance
        """
        if isinstance(json_data, str):
            json_data = json_data.encode("utf-8")
        if serde is None:
            serde = get_serde()
        return _spark_job_from_nested_bytes(json_data, serde)


# =============================================================================
# NESTED FORMAT CLASSES
# =============================================================================


class SparkJobAttributes(AssetAttributes):
    """SparkJob-specific attributes for nested API format."""

    spark_app_name: Union[str, None, UnsetType] = UNSET
    """Name of the Spark app containing this Spark Job For eg. extract_raw_data"""

    spark_master: Union[str, None, UnsetType] = UNSET
    """The Spark master URL eg. local, local[4], or spark://master:7077"""

    spark_run_version: Union[str, None, UnsetType] = UNSET
    """Spark Version for the Spark Job run eg. 3.4.1"""

    spark_run_open_lineage_version: Union[str, None, UnsetType] = UNSET
    """OpenLineage Version of the Spark Job run eg. 1.1.0"""

    spark_run_start_time: Union[int, None, UnsetType] = UNSET
    """Start time of the Spark Job eg. 1695673598218"""

    spark_run_end_time: Union[int, None, UnsetType] = UNSET
    """End time of the Spark Job eg. 1695673598218"""

    spark_run_open_lineage_state: Union[str, None, UnsetType] = UNSET
    """OpenLineage state of the Spark Job run eg. COMPLETE"""


class SparkJobRelationshipAttributes(AssetRelationshipAttributes):
    """SparkJob-specific relationship attributes for nested API format."""

    process: Union[RelatedProcess, None, UnsetType] = UNSET
    """"""

    inputs: Union[list[RelatedCatalog], None, UnsetType] = UNSET
    """"""

    outputs: Union[list[RelatedCatalog], None, UnsetType] = UNSET
    """"""

    spark_orchestrated_by_airflow_assets: Union[
        list[RelatedAirflow], None, UnsetType
    ] = UNSET
    """Airflow assets that execute this spark asset."""


class SparkJobNested(AssetNested):
    """SparkJob in nested API format for high-performance serialization."""

    attributes: Union[SparkJobAttributes, UnsetType] = UNSET
    relationship_attributes: Union[SparkJobRelationshipAttributes, UnsetType] = UNSET
    append_relationship_attributes: Union[SparkJobRelationshipAttributes, UnsetType] = (
        UNSET
    )
    remove_relationship_attributes: Union[SparkJobRelationshipAttributes, UnsetType] = (
        UNSET
    )


# =============================================================================
# CONVERSION FUNCTIONS
# =============================================================================


def _spark_job_to_nested(spark_job: SparkJob) -> SparkJobNested:
    """Convert flat SparkJob to nested format."""
    attrs = SparkJobAttributes(
        spark_app_name=spark_job.spark_app_name,
        spark_master=spark_job.spark_master,
        spark_run_version=spark_job.spark_run_version,
        spark_run_open_lineage_version=spark_job.spark_run_open_lineage_version,
        spark_run_start_time=spark_job.spark_run_start_time,
        spark_run_end_time=spark_job.spark_run_end_time,
        spark_run_open_lineage_state=spark_job.spark_run_open_lineage_state,
    )
    # Categorize relationships by save semantic (REPLACE, APPEND, REMOVE)
    rel_fields: list[str] = [
        "process",
        "inputs",
        "outputs",
        "spark_orchestrated_by_airflow_assets",
    ]
    replace_rels, append_rels, remove_rels = categorize_relationships(
        spark_job, rel_fields, SparkJobRelationshipAttributes
    )
    return SparkJobNested(
        guid=spark_job.guid,
        type_name=spark_job.type_name,
        status=spark_job.status,
        version=spark_job.version,
        create_time=spark_job.create_time,
        update_time=spark_job.update_time,
        created_by=spark_job.created_by,
        updated_by=spark_job.updated_by,
        classifications=spark_job.classifications,
        classification_names=spark_job.classification_names,
        meanings=spark_job.meanings,
        labels=spark_job.labels,
        business_attributes=spark_job.business_attributes,
        custom_attributes=spark_job.custom_attributes,
        pending_tasks=spark_job.pending_tasks,
        proxy=spark_job.proxy,
        is_incomplete=spark_job.is_incomplete,
        provenance_type=spark_job.provenance_type,
        home_id=spark_job.home_id,
        attributes=attrs,
        relationship_attributes=replace_rels,
        append_relationship_attributes=append_rels,
        remove_relationship_attributes=remove_rels,
    )


def _spark_job_from_nested(nested: SparkJobNested) -> SparkJob:
    """Convert nested format to flat SparkJob."""
    attrs = (
        nested.attributes if nested.attributes is not UNSET else SparkJobAttributes()
    )
    # Merge relationships from all three buckets
    rel_fields: list[str] = [
        "process",
        "inputs",
        "outputs",
        "spark_orchestrated_by_airflow_assets",
    ]
    merged_rels = merge_relationships(
        nested.relationship_attributes,
        nested.append_relationship_attributes,
        nested.remove_relationship_attributes,
        rel_fields,
        SparkJobRelationshipAttributes,
    )
    return SparkJob(
        guid=nested.guid,
        type_name=nested.type_name,
        status=nested.status,
        version=nested.version,
        create_time=nested.create_time,
        update_time=nested.update_time,
        created_by=nested.created_by,
        updated_by=nested.updated_by,
        classifications=nested.classifications,
        classification_names=nested.classification_names,
        meanings=nested.meanings,
        labels=nested.labels,
        business_attributes=nested.business_attributes,
        custom_attributes=nested.custom_attributes,
        pending_tasks=nested.pending_tasks,
        proxy=nested.proxy,
        is_incomplete=nested.is_incomplete,
        provenance_type=nested.provenance_type,
        home_id=nested.home_id,
        spark_app_name=attrs.spark_app_name,
        spark_master=attrs.spark_master,
        spark_run_version=attrs.spark_run_version,
        spark_run_open_lineage_version=attrs.spark_run_open_lineage_version,
        spark_run_start_time=attrs.spark_run_start_time,
        spark_run_end_time=attrs.spark_run_end_time,
        spark_run_open_lineage_state=attrs.spark_run_open_lineage_state,
        # Merged relationship attributes
        **merged_rels,
    )


def _spark_job_to_nested_bytes(spark_job: SparkJob, serde: Serde) -> bytes:
    """Convert flat SparkJob to nested JSON bytes."""
    return serde.encode(_spark_job_to_nested(spark_job))


def _spark_job_from_nested_bytes(data: bytes, serde: Serde) -> SparkJob:
    """Convert nested JSON bytes to flat SparkJob."""
    nested = serde.decode(data, SparkJobNested)
    return _spark_job_from_nested(nested)
