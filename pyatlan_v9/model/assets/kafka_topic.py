# Auto-generated by PythonMsgspecRenderer.pkl - DO NOT EDIT
# SPDX-License-Identifier: Apache-2.0
# Copyright 2024 Atlan Pte. Ltd.

"""
KafkaTopic asset model with flattened inheritance.

This module provides:
- KafkaTopic: Flat asset class (easy to use)
- KafkaTopicAttributes: Nested attributes struct (extends AssetAttributes)
- KafkaTopicNested: Nested API format struct
"""

from __future__ import annotations

from typing import Union

import msgspec
from msgspec import UNSET, UnsetType

from pyatlan_v9.model.conversion_utils import (
    build_attributes_kwargs,
    build_flat_kwargs,
    categorize_relationships,
    merge_relationships,
)
from pyatlan_v9.model.serde import Serde, get_serde
from pyatlan_v9.model.transform import register_asset
from pyatlan_v9.utils import init_guid, validate_required_fields

from .asset import Asset, AssetAttributes, AssetNested, AssetRelationshipAttributes
from .kafka_related import RelatedKafkaConsumerGroup

# =============================================================================
# FLAT ASSET CLASS
# =============================================================================


@register_asset
class KafkaTopic(Asset):
    """
    Instance of a Kafka Topic in Atlan.
    """

    # Override type_name with KafkaTopic-specific default
    type_name: Union[str, UnsetType] = "KafkaTopic"

    kafka_topic_is_internal: Union[bool, None, UnsetType] = UNSET
    """Whether this topic is an internal topic (true) or not (false)."""

    kafka_topic_compression_type: Union[str, None, UnsetType] = UNSET
    """Type of compression used for this topic."""

    kafka_topic_replication_factor: Union[int, None, UnsetType] = UNSET
    """Replication factor for this topic."""

    kafka_topic_segment_bytes: Union[int, None, UnsetType] = UNSET
    """Segment size for this topic."""

    kafka_topic_retention_time_in_ms: Union[int, None, UnsetType] = UNSET
    """Amount of time messages will be retained in this topic, in milliseconds."""

    kafka_topic_partitions_count: Union[int, None, UnsetType] = UNSET
    """Number of partitions for this topic."""

    kafka_topic_size_in_bytes: Union[int, None, UnsetType] = UNSET
    """Size of this topic, in bytes."""

    kafka_topic_record_count: Union[int, None, UnsetType] = UNSET
    """Number of (unexpired) messages in this topic."""

    kafka_topic_cleanup_policy: Union[str, None, UnsetType] = UNSET
    """Cleanup policy for this topic."""

    kafka_topic_log_cleanup_policy: Union[str, None, UnsetType] = UNSET
    """Comma seperated Cleanup policy for this topic."""

    kafka_consumer_groups: Union[list[RelatedKafkaConsumerGroup], None, UnsetType] = (
        UNSET
    )
    """Consumer groups subscribed to this topic."""

    # =========================================================================
    # Convenience Methods
    # =========================================================================

    @classmethod
    @init_guid
    def creator(
        cls,
        *,
        name: str,
        connection_qualified_name: str,
    ) -> "KafkaTopic":
        validate_required_fields(
            ["name", "connection_qualified_name"],
            [name, connection_qualified_name],
        )
        fields = connection_qualified_name.split("/")
        connector_name = fields[1] if len(fields) > 1 else None
        qualified_name = f"{connection_qualified_name}/topic/{name}"
        return cls(
            name=name,
            qualified_name=qualified_name,
            connector_name=connector_name,
            connection_qualified_name=connection_qualified_name,
        )

    @classmethod
    def create(cls, **kwargs) -> "KafkaTopic":
        return cls.creator(**kwargs)

    @classmethod
    def create_for_modification(cls, **kwargs) -> "KafkaTopic":
        return cls.updater(**kwargs)

    # =========================================================================
    # Optimized Serialization Methods (override Asset base class)
    # =========================================================================

    def to_json(self, nested: bool = True, serde: Serde | None = None) -> str:
        """
        Convert to JSON string using optimized nested struct serialization.

        Args:
            nested: If True (default), use nested API format. If False, use flat format.
            serde: Optional Serde instance for encoder reuse. Uses shared singleton if None.

        Returns:
            JSON string representation
        """
        if serde is None:
            serde = get_serde()
        if nested:
            return _kafka_topic_to_nested_bytes(self, serde).decode("utf-8")
        else:
            return serde.encode(self).decode("utf-8")

    @staticmethod
    def from_json(
        json_data: Union[str, bytes], serde: Serde | None = None
    ) -> "KafkaTopic":
        """
        Create from JSON string or bytes using optimized nested struct deserialization.

        Args:
            json_data: JSON string or bytes to deserialize
            serde: Optional Serde instance for decoder reuse. Uses shared singleton if None.

        Returns:
            KafkaTopic instance
        """
        if isinstance(json_data, str):
            json_data = json_data.encode("utf-8")
        if serde is None:
            serde = get_serde()
        return _kafka_topic_from_nested_bytes(json_data, serde)


# =============================================================================
# NESTED FORMAT CLASSES
# =============================================================================


class KafkaTopicAttributes(AssetAttributes):
    """KafkaTopic-specific attributes for nested API format."""

    kafka_topic_is_internal: Union[bool, None, UnsetType] = UNSET
    """Whether this topic is an internal topic (true) or not (false)."""

    kafka_topic_compression_type: Union[str, None, UnsetType] = UNSET
    """Type of compression used for this topic."""

    kafka_topic_replication_factor: Union[int, None, UnsetType] = UNSET
    """Replication factor for this topic."""

    kafka_topic_segment_bytes: Union[int, None, UnsetType] = UNSET
    """Segment size for this topic."""

    kafka_topic_retention_time_in_ms: Union[int, None, UnsetType] = UNSET
    """Amount of time messages will be retained in this topic, in milliseconds."""

    kafka_topic_partitions_count: Union[int, None, UnsetType] = UNSET
    """Number of partitions for this topic."""

    kafka_topic_size_in_bytes: Union[int, None, UnsetType] = UNSET
    """Size of this topic, in bytes."""

    kafka_topic_record_count: Union[int, None, UnsetType] = UNSET
    """Number of (unexpired) messages in this topic."""

    kafka_topic_cleanup_policy: Union[str, None, UnsetType] = UNSET
    """Cleanup policy for this topic."""

    kafka_topic_log_cleanup_policy: Union[str, None, UnsetType] = UNSET
    """Comma seperated Cleanup policy for this topic."""


class KafkaTopicRelationshipAttributes(AssetRelationshipAttributes):
    """KafkaTopic-specific relationship attributes for nested API format."""

    kafka_consumer_groups: Union[list[RelatedKafkaConsumerGroup], None, UnsetType] = (
        UNSET
    )
    """Consumer groups subscribed to this topic."""


class KafkaTopicNested(AssetNested):
    """KafkaTopic in nested API format for high-performance serialization."""

    attributes: Union[KafkaTopicAttributes, UnsetType] = UNSET
    relationship_attributes: Union[KafkaTopicRelationshipAttributes, UnsetType] = UNSET
    append_relationship_attributes: Union[
        KafkaTopicRelationshipAttributes, UnsetType
    ] = UNSET
    remove_relationship_attributes: Union[
        KafkaTopicRelationshipAttributes, UnsetType
    ] = UNSET


# =============================================================================
# CONVERSION FUNCTIONS
# =============================================================================


def _kafka_topic_to_nested(kafka_topic: KafkaTopic) -> KafkaTopicNested:
    """Convert flat KafkaTopic to nested format."""
    attrs_kwargs = build_attributes_kwargs(kafka_topic, KafkaTopicAttributes)
    attrs = KafkaTopicAttributes(**attrs_kwargs)
    # Categorize relationships by save semantic (REPLACE, APPEND, REMOVE)
    rel_fields: list[str] = ["kafka_consumer_groups"]
    replace_rels, append_rels, remove_rels = categorize_relationships(
        kafka_topic, rel_fields, KafkaTopicRelationshipAttributes
    )
    # Build top-level nested kwargs dynamically from AssetNested fields
    nested_field_names = {
        f.name
        for f in msgspec.structs.fields(AssetNested)
        if f.name
        not in (
            "attributes",
            "relationship_attributes",
            "append_relationship_attributes",
            "remove_relationship_attributes",
        )
    }
    nested_kwargs = {
        name: getattr(kafka_topic, name)
        for name in nested_field_names
        if hasattr(kafka_topic, name)
    }
    return KafkaTopicNested(
        **nested_kwargs,
        attributes=attrs,
        relationship_attributes=replace_rels,
        append_relationship_attributes=append_rels,
        remove_relationship_attributes=remove_rels,
    )


def _kafka_topic_from_nested(nested: KafkaTopicNested) -> KafkaTopic:
    """Convert nested format to flat KafkaTopic."""
    attrs = (
        nested.attributes if nested.attributes is not UNSET else KafkaTopicAttributes()
    )
    # Merge relationships from all three buckets
    rel_fields: list[str] = ["kafka_consumer_groups"]
    merged_rels = merge_relationships(
        nested.relationship_attributes,
        nested.append_relationship_attributes,
        nested.remove_relationship_attributes,
        rel_fields,
        KafkaTopicRelationshipAttributes,
    )
    kwargs = build_flat_kwargs(
        nested, attrs, merged_rels, AssetNested, KafkaTopicAttributes
    )
    return KafkaTopic(**kwargs)


def _kafka_topic_to_nested_bytes(kafka_topic: KafkaTopic, serde: Serde) -> bytes:
    """Convert flat KafkaTopic to nested JSON bytes."""
    return serde.encode(_kafka_topic_to_nested(kafka_topic))


def _kafka_topic_from_nested_bytes(data: bytes, serde: Serde) -> KafkaTopic:
    """Convert nested JSON bytes to flat KafkaTopic."""
    nested = serde.decode(data, KafkaTopicNested)
    return _kafka_topic_from_nested(nested)
