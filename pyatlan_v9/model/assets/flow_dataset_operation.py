# Auto-generated by PythonMsgspecRenderer.pkl - DO NOT EDIT
# SPDX-License-Identifier: Apache-2.0
# Copyright 2024 Atlan Pte. Ltd.

"""
FlowDatasetOperation asset model with flattened inheritance.

This module provides:
- FlowDatasetOperation: Flat asset class (easy to use)
- FlowDatasetOperationAttributes: Nested attributes struct (extends AssetAttributes)
- FlowDatasetOperationNested: Nested API format struct
"""

from __future__ import annotations

from typing import Union

from msgspec import UNSET, UnsetType

from pyatlan_v9.model.conversion_utils import (
    categorize_relationships,
    merge_relationships,
)
from pyatlan_v9.model.serde import Serde, get_serde
from pyatlan_v9.model.transform import register_asset

from .asset import Asset, AssetAttributes, AssetNested, AssetRelationshipAttributes
from .flow_related import RelatedFlowControlOperation, RelatedFlowReusableUnit

# =============================================================================
# FLAT ASSET CLASS
# =============================================================================


@register_asset
class FlowDatasetOperation(Asset):
    """
    A nested data operation that uses at least one ephemeral dataset as either an input or output.
    """

    # Override type_name with FlowDatasetOperation-specific default
    type_name: Union[str, UnsetType] = "FlowDatasetOperation"

    flow_started_at: Union[int, None, UnsetType] = UNSET
    """Date and time at which this point in the data processing or orchestration started."""

    flow_finished_at: Union[int, None, UnsetType] = UNSET
    """Date and time at which this point in the data processing or orchestration finished."""

    flow_status: Union[str, None, UnsetType] = UNSET
    """Overall status of this point in the data processing or orchestration."""

    flow_schedule: Union[str, None, UnsetType] = UNSET
    """Schedule for this point in the data processing or orchestration."""

    flow_project_name: Union[str, None, UnsetType] = UNSET
    """Simple name of the project in which this asset is contained."""

    flow_project_qualified_name: Union[str, None, UnsetType] = UNSET
    """Unique name of the project in which this asset is contained."""

    flow_folder_name: Union[str, None, UnsetType] = UNSET
    """Simple name of the folder in which this asset is contained."""

    flow_folder_qualified_name: Union[str, None, UnsetType] = UNSET
    """Unique name of the folder in which this asset is contained."""

    flow_reusable_unit_name: Union[str, None, UnsetType] = UNSET
    """Simple name of the reusable grouping of operations in which this ephemeral data is contained."""

    flow_reusable_unit_qualified_name: Union[str, None, UnsetType] = UNSET
    """Unique name of the reusable grouping of operations in which this ephemeral data is contained."""

    flow_id: Union[str, None, UnsetType] = UNSET
    """Unique ID for this flow asset, which will remain constant throughout the lifecycle of the asset."""

    flow_run_id: Union[str, None, UnsetType] = UNSET
    """Unique ID of the flow run, which could change on subsequent runs of the same flow."""

    flow_error_message: Union[str, None, UnsetType] = UNSET
    """Optional error message of the flow run."""

    flow_input_parameters: Union[dict[str, str], None, UnsetType] = UNSET
    """Input parameters for the flow run."""

    code: Union[str, None, UnsetType] = UNSET
    """Code that ran within the process."""

    sql: Union[str, None, UnsetType] = UNSET
    """SQL query that ran to produce the outputs."""

    parent_connection_process_qualified_name: Union[list[str], None, UnsetType] = UNSET
    """"""

    ast: Union[str, None, UnsetType] = UNSET
    """Parsed AST of the code or SQL statements that describe the logic of this process."""

    additional_etl_context: Union[str, None, UnsetType] = UNSET
    """Additional Context of the ETL pipeline/notebook which creates the process."""

    ai_dataset_type: Union[str, None, UnsetType] = UNSET
    """Dataset type for AI Model - dataset process."""

    flow_orchestrated_by: Union[RelatedFlowControlOperation, None, UnsetType] = UNSET
    """Orchestrated control operation that ran these data flows (process)."""

    flow_reusable_unit: Union[RelatedFlowReusableUnit, None, UnsetType] = UNSET
    """Reusable unit of dataset operations that are all executed together."""

    # =========================================================================
    # Optimized Serialization Methods (override Asset base class)
    # =========================================================================

    def to_json(self, nested: bool = True, serde: Serde | None = None) -> str:
        """
        Convert to JSON string using optimized nested struct serialization.

        Args:
            nested: If True (default), use nested API format. If False, use flat format.
            serde: Optional Serde instance for encoder reuse. Uses shared singleton if None.

        Returns:
            JSON string representation
        """
        if serde is None:
            serde = get_serde()
        if nested:
            return _flow_dataset_operation_to_nested_bytes(self, serde).decode("utf-8")
        else:
            return serde.encode(self).decode("utf-8")

    @staticmethod
    def from_json(
        json_data: Union[str, bytes], serde: Serde | None = None
    ) -> "FlowDatasetOperation":
        """
        Create from JSON string or bytes using optimized nested struct deserialization.

        Args:
            json_data: JSON string or bytes to deserialize
            serde: Optional Serde instance for decoder reuse. Uses shared singleton if None.

        Returns:
            FlowDatasetOperation instance
        """
        if isinstance(json_data, str):
            json_data = json_data.encode("utf-8")
        if serde is None:
            serde = get_serde()
        return _flow_dataset_operation_from_nested_bytes(json_data, serde)


# =============================================================================
# NESTED FORMAT CLASSES
# =============================================================================


class FlowDatasetOperationAttributes(AssetAttributes):
    """FlowDatasetOperation-specific attributes for nested API format."""

    flow_started_at: Union[int, None, UnsetType] = UNSET
    """Date and time at which this point in the data processing or orchestration started."""

    flow_finished_at: Union[int, None, UnsetType] = UNSET
    """Date and time at which this point in the data processing or orchestration finished."""

    flow_status: Union[str, None, UnsetType] = UNSET
    """Overall status of this point in the data processing or orchestration."""

    flow_schedule: Union[str, None, UnsetType] = UNSET
    """Schedule for this point in the data processing or orchestration."""

    flow_project_name: Union[str, None, UnsetType] = UNSET
    """Simple name of the project in which this asset is contained."""

    flow_project_qualified_name: Union[str, None, UnsetType] = UNSET
    """Unique name of the project in which this asset is contained."""

    flow_folder_name: Union[str, None, UnsetType] = UNSET
    """Simple name of the folder in which this asset is contained."""

    flow_folder_qualified_name: Union[str, None, UnsetType] = UNSET
    """Unique name of the folder in which this asset is contained."""

    flow_reusable_unit_name: Union[str, None, UnsetType] = UNSET
    """Simple name of the reusable grouping of operations in which this ephemeral data is contained."""

    flow_reusable_unit_qualified_name: Union[str, None, UnsetType] = UNSET
    """Unique name of the reusable grouping of operations in which this ephemeral data is contained."""

    flow_id: Union[str, None, UnsetType] = UNSET
    """Unique ID for this flow asset, which will remain constant throughout the lifecycle of the asset."""

    flow_run_id: Union[str, None, UnsetType] = UNSET
    """Unique ID of the flow run, which could change on subsequent runs of the same flow."""

    flow_error_message: Union[str, None, UnsetType] = UNSET
    """Optional error message of the flow run."""

    flow_input_parameters: Union[dict[str, str], None, UnsetType] = UNSET
    """Input parameters for the flow run."""

    code: Union[str, None, UnsetType] = UNSET
    """Code that ran within the process."""

    sql: Union[str, None, UnsetType] = UNSET
    """SQL query that ran to produce the outputs."""

    parent_connection_process_qualified_name: Union[list[str], None, UnsetType] = UNSET
    """"""

    ast: Union[str, None, UnsetType] = UNSET
    """Parsed AST of the code or SQL statements that describe the logic of this process."""

    additional_etl_context: Union[str, None, UnsetType] = UNSET
    """Additional Context of the ETL pipeline/notebook which creates the process."""

    ai_dataset_type: Union[str, None, UnsetType] = UNSET
    """Dataset type for AI Model - dataset process."""


class FlowDatasetOperationRelationshipAttributes(AssetRelationshipAttributes):
    """FlowDatasetOperation-specific relationship attributes for nested API format."""

    flow_orchestrated_by: Union[RelatedFlowControlOperation, None, UnsetType] = UNSET
    """Orchestrated control operation that ran these data flows (process)."""

    flow_reusable_unit: Union[RelatedFlowReusableUnit, None, UnsetType] = UNSET
    """Reusable unit of dataset operations that are all executed together."""


class FlowDatasetOperationNested(AssetNested):
    """FlowDatasetOperation in nested API format for high-performance serialization."""

    attributes: Union[FlowDatasetOperationAttributes, UnsetType] = UNSET
    relationship_attributes: Union[
        FlowDatasetOperationRelationshipAttributes, UnsetType
    ] = UNSET
    append_relationship_attributes: Union[
        FlowDatasetOperationRelationshipAttributes, UnsetType
    ] = UNSET
    remove_relationship_attributes: Union[
        FlowDatasetOperationRelationshipAttributes, UnsetType
    ] = UNSET


# =============================================================================
# CONVERSION FUNCTIONS
# =============================================================================


def _flow_dataset_operation_to_nested(
    flow_dataset_operation: FlowDatasetOperation,
) -> FlowDatasetOperationNested:
    """Convert flat FlowDatasetOperation to nested format."""
    attrs = FlowDatasetOperationAttributes(
        flow_started_at=flow_dataset_operation.flow_started_at,
        flow_finished_at=flow_dataset_operation.flow_finished_at,
        flow_status=flow_dataset_operation.flow_status,
        flow_schedule=flow_dataset_operation.flow_schedule,
        flow_project_name=flow_dataset_operation.flow_project_name,
        flow_project_qualified_name=flow_dataset_operation.flow_project_qualified_name,
        flow_folder_name=flow_dataset_operation.flow_folder_name,
        flow_folder_qualified_name=flow_dataset_operation.flow_folder_qualified_name,
        flow_reusable_unit_name=flow_dataset_operation.flow_reusable_unit_name,
        flow_reusable_unit_qualified_name=flow_dataset_operation.flow_reusable_unit_qualified_name,
        flow_id=flow_dataset_operation.flow_id,
        flow_run_id=flow_dataset_operation.flow_run_id,
        flow_error_message=flow_dataset_operation.flow_error_message,
        flow_input_parameters=flow_dataset_operation.flow_input_parameters,
        code=flow_dataset_operation.code,
        sql=flow_dataset_operation.sql,
        parent_connection_process_qualified_name=flow_dataset_operation.parent_connection_process_qualified_name,
        ast=flow_dataset_operation.ast,
        additional_etl_context=flow_dataset_operation.additional_etl_context,
        ai_dataset_type=flow_dataset_operation.ai_dataset_type,
    )
    # Categorize relationships by save semantic (REPLACE, APPEND, REMOVE)
    rel_fields: list[str] = ["flow_orchestrated_by", "flow_reusable_unit"]
    replace_rels, append_rels, remove_rels = categorize_relationships(
        flow_dataset_operation, rel_fields, FlowDatasetOperationRelationshipAttributes
    )
    return FlowDatasetOperationNested(
        guid=flow_dataset_operation.guid,
        type_name=flow_dataset_operation.type_name,
        status=flow_dataset_operation.status,
        version=flow_dataset_operation.version,
        create_time=flow_dataset_operation.create_time,
        update_time=flow_dataset_operation.update_time,
        created_by=flow_dataset_operation.created_by,
        updated_by=flow_dataset_operation.updated_by,
        classifications=flow_dataset_operation.classifications,
        classification_names=flow_dataset_operation.classification_names,
        meanings=flow_dataset_operation.meanings,
        labels=flow_dataset_operation.labels,
        business_attributes=flow_dataset_operation.business_attributes,
        custom_attributes=flow_dataset_operation.custom_attributes,
        pending_tasks=flow_dataset_operation.pending_tasks,
        proxy=flow_dataset_operation.proxy,
        is_incomplete=flow_dataset_operation.is_incomplete,
        provenance_type=flow_dataset_operation.provenance_type,
        home_id=flow_dataset_operation.home_id,
        attributes=attrs,
        relationship_attributes=replace_rels,
        append_relationship_attributes=append_rels,
        remove_relationship_attributes=remove_rels,
    )


def _flow_dataset_operation_from_nested(
    nested: FlowDatasetOperationNested,
) -> FlowDatasetOperation:
    """Convert nested format to flat FlowDatasetOperation."""
    attrs = (
        nested.attributes
        if nested.attributes is not UNSET
        else FlowDatasetOperationAttributes()
    )
    # Merge relationships from all three buckets
    rel_fields: list[str] = ["flow_orchestrated_by", "flow_reusable_unit"]
    merged_rels = merge_relationships(
        nested.relationship_attributes,
        nested.append_relationship_attributes,
        nested.remove_relationship_attributes,
        rel_fields,
        FlowDatasetOperationRelationshipAttributes,
    )
    return FlowDatasetOperation(
        guid=nested.guid,
        type_name=nested.type_name,
        status=nested.status,
        version=nested.version,
        create_time=nested.create_time,
        update_time=nested.update_time,
        created_by=nested.created_by,
        updated_by=nested.updated_by,
        classifications=nested.classifications,
        classification_names=nested.classification_names,
        meanings=nested.meanings,
        labels=nested.labels,
        business_attributes=nested.business_attributes,
        custom_attributes=nested.custom_attributes,
        pending_tasks=nested.pending_tasks,
        proxy=nested.proxy,
        is_incomplete=nested.is_incomplete,
        provenance_type=nested.provenance_type,
        home_id=nested.home_id,
        flow_started_at=attrs.flow_started_at,
        flow_finished_at=attrs.flow_finished_at,
        flow_status=attrs.flow_status,
        flow_schedule=attrs.flow_schedule,
        flow_project_name=attrs.flow_project_name,
        flow_project_qualified_name=attrs.flow_project_qualified_name,
        flow_folder_name=attrs.flow_folder_name,
        flow_folder_qualified_name=attrs.flow_folder_qualified_name,
        flow_reusable_unit_name=attrs.flow_reusable_unit_name,
        flow_reusable_unit_qualified_name=attrs.flow_reusable_unit_qualified_name,
        flow_id=attrs.flow_id,
        flow_run_id=attrs.flow_run_id,
        flow_error_message=attrs.flow_error_message,
        flow_input_parameters=attrs.flow_input_parameters,
        code=attrs.code,
        sql=attrs.sql,
        parent_connection_process_qualified_name=attrs.parent_connection_process_qualified_name,
        ast=attrs.ast,
        additional_etl_context=attrs.additional_etl_context,
        ai_dataset_type=attrs.ai_dataset_type,
        # Merged relationship attributes
        **merged_rels,
    )


def _flow_dataset_operation_to_nested_bytes(
    flow_dataset_operation: FlowDatasetOperation, serde: Serde
) -> bytes:
    """Convert flat FlowDatasetOperation to nested JSON bytes."""
    return serde.encode(_flow_dataset_operation_to_nested(flow_dataset_operation))


def _flow_dataset_operation_from_nested_bytes(
    data: bytes, serde: Serde
) -> FlowDatasetOperation:
    """Convert nested JSON bytes to flat FlowDatasetOperation."""
    nested = serde.decode(data, FlowDatasetOperationNested)
    return _flow_dataset_operation_from_nested(nested)
